{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2099bd",
   "metadata": {},
   "source": [
    "Code reference - https://www.kaggle.com/code/balraj98/context-encoder-gan-for-image-inpainting-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc21bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, sys\n",
    "import glob, itertools\n",
    "import argparse, random\n",
    "import sewar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1124406",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted(list(os.listdir('/home/k4agrawal/Efficient_Image_Reconstruction/StyleGAN.pytorch/ffhq')))[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec132d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "allImages = []\n",
    "root = '/home/k4agrawal/Efficient_Image_Reconstruction/StyleGAN.pytorch/ffhq'\n",
    "for folder in folders:\n",
    "    allImages.extend(sorted(glob.glob(\"%s/%s/*.png\" %(root,folder))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b89ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImage, testImage = train_test_split(allImages, test_size=0.2, random_state=42)\n",
    "valImage, testImage = train_test_split(testImage, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6849533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.img_size = img_size\n",
    "        self.mask_size = mask_size\n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        # for folder in folders:\n",
    "        #    self.files.extend(sorted(glob.glob(\"%s/%s/*.png\" %(root,folder))))\n",
    "        # self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n",
    "\n",
    "    def apply_random_mask(self, img):\n",
    "        \"\"\"Randomly masks image\"\"\"\n",
    "        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n",
    "        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
    "        masked_part = img[:, y1:y2, x1:x2]\n",
    "        masked_img = img.clone()\n",
    "        masked_img[:, y1:y2, x1:x2] = 1\n",
    "\n",
    "        return masked_img, masked_part\n",
    "\n",
    "    def apply_center_mask(self, img):\n",
    "        \"\"\"Mask center part of image\"\"\"\n",
    "        # Get upper-left pixel coordinate\n",
    "        i = (self.img_size - self.mask_size) // 2\n",
    "        masked_img = img.clone()\n",
    "        masked_part = masked_img[:, i : i + self.mask_size, i : i + self.mask_size]\n",
    "        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
    "\n",
    "        return masked_img, masked_part, i\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img = self.transform(img)\n",
    "        if(self.mode==\"train\"):\n",
    "            masked_img, aux = self.apply_random_mask(img)\n",
    "            return img, masked_img, aux\n",
    "        else:\n",
    "            masked_img, masked_part, i = self.apply_center_mask(img)\n",
    "            return img, masked_img, masked_part, i\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a664b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize((128, 128), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "trainDL = DataLoader(\n",
    "    ImageDataset(files=trainImage, transforms_=transforms_, mode=\"train\"),\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "valDL = DataLoader(\n",
    "    ImageDataset(files=valImage, transforms_=transforms_, mode=\"train\"),\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "testDL = DataLoader(\n",
    "    ImageDataset(files=testImage, transforms_=transforms_, mode=\"test\"),\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0219402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def downsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        def upsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.ReLU())\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *downsample(channels, 64, normalize=False),\n",
    "            *downsample(64, 64),\n",
    "            *downsample(64, 128),\n",
    "            *downsample(128, 256),\n",
    "            *downsample(256, 512),\n",
    "            nn.Conv2d(512, 4000, 1),\n",
    "            *upsample(4000, 512),\n",
    "            *upsample(512, 256),\n",
    "            *upsample(256, 128),\n",
    "            *upsample(128, 64),\n",
    "            nn.Conv2d(64, channels, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a89cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# def save_sample(batches_done):\n",
    "    \n",
    "\n",
    "    \n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "pixelwise_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(channels=3)\n",
    "discriminator = Discriminator(channels=3)\n",
    "\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "adversarial_loss.cuda()\n",
    "pixelwise_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a4d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_h, patch_w = int(64 / 2 ** 3), int(64 / 2 ** 3)\n",
    "patch = (1, patch_h, patch_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e7cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6215234d66f42b2abe3c8a8635da36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a435ae37f02444b8e99d815ca4e87af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 0 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val adv loss: 561.456576, val pixel loss: 142.622745, val disc loss: 1.808908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e19e3b9fc3f42159e3b6cc3c2e8ed89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc722cb3c9c42d3b51eb5cad56253ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 1 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, val adv loss: 554.029182, val pixel loss: 144.708080, val disc loss: 1.609072\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6909874f9d473eb5b0235d35572297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4692eee0da460993ae7026b819bf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 2 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, val adv loss: 580.227565, val pixel loss: 126.584137, val disc loss: 0.295926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4754ecbd814aad97c2830946b5718a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f062915fc8485c9fc5a022d45d8868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 3 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, val adv loss: 578.140746, val pixel loss: 123.464378, val disc loss: 0.154754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644240c77b13497ebe0cbbda9690025e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53727a2315dd47eda985a331e1473a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 4 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, val adv loss: 575.521801, val pixel loss: 120.222854, val disc loss: 0.391044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc3a65a8c1748f4bb54e1c77ad60b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540469cbbebb4b3680a12f60ace25471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 5 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, val adv loss: 579.301153, val pixel loss: 118.336148, val disc loss: 0.074646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f43f43dee44368ae7adaa5caab0cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b2f0ac001042a9a7b4f8cf4df99609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 6 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, val adv loss: 583.143052, val pixel loss: 116.269866, val disc loss: 0.071606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bf89cfeebf4231bea0e56f8e58d630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746d4930f3c7497b93ff5067c92674ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 7 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, val adv loss: 579.654421, val pixel loss: 114.635081, val disc loss: 0.211934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd2bd561fb243f1a50f8ce485cd83b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d7bf0ee59b4dd88490a567a52735a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 8 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, val adv loss: 584.002416, val pixel loss: 113.146289, val disc loss: 0.045975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f113878bac664a238302812aadbf5328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45725dbd711b4183bcc8589ada892cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 9 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, val adv loss: 581.280777, val pixel loss: 111.342367, val disc loss: 0.098447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c0fb50ea874535bc6321e5b1ba0294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43312426411435ca653de1a0a77e7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 10 :   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, val adv loss: 594.953227, val pixel loss: 112.666951, val disc loss: 0.071743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50b0d0c0d1e48cb96f8cf9605033e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 11 :   0%|          | 0/4667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_adv_losses, gen_pixel_losses, disc_losses, counter = [], [], [], []\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    ### Training ###\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    gen_adv_loss, gen_pixel_loss, disc_loss = 0, 0, 0\n",
    "    tqdm_bar = tqdm(trainDL, desc=f'Training Epoch {epoch} ', total=int(len(trainDL)))\n",
    "    for i, (imgs, masked_imgs, masked_parts) in enumerate(tqdm_bar):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        masked_imgs = Variable(masked_imgs.type(Tensor))\n",
    "        masked_parts = Variable(masked_parts.type(Tensor))\n",
    "\n",
    "        ## Train Generator ##\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_parts = generator(masked_imgs)\n",
    "\n",
    "        # Adversarial and pixelwise loss\n",
    "        g_adv = adversarial_loss(discriminator(gen_parts), valid)\n",
    "        # print(gen_parts.shape, masked_parts.shape)\n",
    "        g_pixel = pixelwise_loss(gen_parts, masked_parts)\n",
    "        # Total loss\n",
    "        g_loss = 0.001 * g_adv + 0.999 * g_pixel\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ## Train Discriminator ##\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(masked_parts), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_parts.detach()), fake)\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        gen_adv_loss, gen_pixel_loss, disc_loss\n",
    "        gen_adv_losses, gen_pixel_losses, disc_losses, counter\n",
    "        \n",
    "        gen_adv_loss += g_adv.item()\n",
    "        gen_pixel_loss += g_pixel.item()\n",
    "        gen_adv_losses.append(g_adv.item())\n",
    "        gen_pixel_losses.append(g_pixel.item())\n",
    "        disc_loss += d_loss.item()\n",
    "        disc_losses.append(d_loss.item())\n",
    "        counter.append(i*12 + imgs.size(0) + epoch*len(trainDL.dataset))\n",
    "        tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss/(i+1), gen_pixel_loss=gen_pixel_loss/(i+1), disc_loss=disc_loss/(i+1))\n",
    "        \n",
    "        # Generate sample at sample interval\n",
    "        batches_done = epoch * len(trainDL) + i\n",
    "#         if batches_done % 1000 == 0:\n",
    "#             save_sample(batches_done)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "        adv_val_loss, pixel_val_loss, disc_val_loss = 0, 0, 0\n",
    "        val_tqdm_bar = tqdm(valDL, desc=f'Val Epoch {epoch} ', total=int(len(valDL)))\n",
    "        for i, (imgs, masked_imgs, masked_parts) in enumerate(val_tqdm_bar):\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n",
    "\n",
    "            # Configure input\n",
    "            imgs = Variable(imgs.type(Tensor))\n",
    "            masked_imgs = Variable(masked_imgs.type(Tensor))\n",
    "            masked_parts = Variable(masked_parts.type(Tensor))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_parts = generator(masked_imgs)\n",
    "\n",
    "            # Adversarial and pixelwise loss\n",
    "            g_adv = adversarial_loss(discriminator(gen_parts), valid)\n",
    "            g_pixel = pixelwise_loss(gen_parts, masked_parts)\n",
    "            # Total loss\n",
    "            g_loss = 0.001 * g_adv + 0.999 * g_pixel\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = adversarial_loss(discriminator(masked_parts), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_parts.detach()), fake)\n",
    "            d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        \n",
    "            adv_val_loss += g_adv.item()\n",
    "            pixel_val_loss += g_pixel.item()\n",
    "            disc_val_loss += d_loss.item()\n",
    "            val_tqdm_bar.set_postfix(adv_val_loss=adv_val_loss/(i+1), pixel_val_loss=pixel_val_loss/(i+1), disc_val_loss=disc_val_loss/(i+1))\n",
    "        print(\"Epoch: %d, val adv loss: %f, val pixel loss: %f, val disc loss: %f\"%(epoch, adv_val_loss, pixel_val_loss, disc_val_loss))\n",
    "\n",
    "        if((0.001 * adv_val_loss + 0.999 * pixel_val_loss) < best_loss):\n",
    "            best_loss = 0.001 * adv_val_loss + 0.999 * pixel_val_loss\n",
    "            torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n",
    "            torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db480a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "import cv2\n",
    "def normalize_img(img):\n",
    "    norm_image = cv2.normalize(img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n",
    "    norm_image = norm_image.astype(np.uint8)\n",
    "    return norm_image\n",
    "\n",
    "with torch.no_grad():\n",
    "    generator.load_state_dict(torch.load('saved_models/generator.pth'))\n",
    "    discriminator.load_state_dict(torch.load('saved_models/discriminator.pth'))\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    test_tqdm_bar = tqdm(testDL, desc=f'Test Epoch 1 ', total=int(len(testDL)))\n",
    "    pixel_test_loss = 0\n",
    "    for step, (imgs, masked_imgs, masked_parts, i) in enumerate(test_tqdm_bar):\n",
    "\n",
    "#     samples, masked_samples, i = next(iter(testDL))\n",
    "        samples = Variable(imgs.type(Tensor))\n",
    "        masked_samples = Variable(masked_imgs.type(Tensor))\n",
    "        masked_parts = Variable(masked_parts.type(Tensor))\n",
    "        i = i[0].item()  # Upper-left coordinate of mask\n",
    "        # Generate inpainted image\n",
    "        gen_mask = generator(masked_samples)\n",
    "    \n",
    "        g_pixel = pixelwise_loss(gen_mask, masked_parts)\n",
    "        # print(gen_mask.shape, coords)\n",
    "        filled_samples = masked_samples.clone()\n",
    "        filled_samples[:, :, i : i + 64, i : i + 64] = gen_mask\n",
    "        # Save sample\n",
    "        sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2)\n",
    "        psnr = 0\n",
    "        ssim = 0\n",
    "        for i in range(12):\n",
    "            pred = normalize_img(filled_samples[i].permute(1,2,0).cpu().numpy())\n",
    "            gt = normalize_img(samples[i].permute(1,2,0).cpu().numpy())\n",
    "            # print(pred.shape, gt.shape)\n",
    "            psnr_score = sewar.psnr(pred,gt)\n",
    "            # print(psnr_score)\n",
    "            #ssim\n",
    "            ssim_score = sewar.ssim(pred,gt)[0]\n",
    "            psnr += psnr_score\n",
    "            ssim += ssim_score\n",
    "        print(\"psnr_score: %f, ssim_score: %f\"%(psnr/12, ssim/12))\n",
    "        pixel_test_loss += g_pixel.item()\n",
    "        if(step % 50 == 0):\n",
    "            save_image(sample, \"images/%d.png\" % step, nrow=6, normalize=True)\n",
    "    print(\"final pixel loss: {:.4f}\".format(pixel_test_loss/int(len(testDL))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a533ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
