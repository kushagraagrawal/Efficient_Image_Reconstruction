{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2099bd",
   "metadata": {},
   "source": [
    "Code reference - https://www.kaggle.com/code/balraj98/context-encoder-gan-for-image-inpainting-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc21bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, sys\n",
    "import glob, itertools\n",
    "import argparse, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg19\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1124406",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = sorted(list(os.listdir('ffhq')))[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6849533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, folders, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.img_size = img_size\n",
    "        self.mask_size = mask_size\n",
    "        self.mode = mode\n",
    "        self.files = []\n",
    "        for folder in folders:\n",
    "            self.files.extend(sorted(glob.glob(\"%s/%s/*.png\" %(root,folder))))\n",
    "        self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n",
    "\n",
    "    def apply_random_mask(self, img):\n",
    "        \"\"\"Randomly masks image\"\"\"\n",
    "        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n",
    "        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n",
    "        masked_part = img[:, y1:y2, x1:x2]\n",
    "        masked_img = img.clone()\n",
    "        masked_img[:, y1:y2, x1:x2] = 1\n",
    "\n",
    "        return masked_img, masked_part\n",
    "\n",
    "    def apply_center_mask(self, img):\n",
    "        \"\"\"Mask center part of image\"\"\"\n",
    "        # Get upper-left pixel coordinate\n",
    "        i = (self.img_size - self.mask_size) // 2\n",
    "        masked_img = img.clone()\n",
    "        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n",
    "\n",
    "        return masked_img, i\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        img = self.transform(img)\n",
    "        if self.mode == \"train\":\n",
    "            # For training data perform random mask\n",
    "            masked_img, aux = self.apply_random_mask(img)\n",
    "        else:\n",
    "            # For test data mask the center of the image\n",
    "            masked_img, aux = self.apply_center_mask(img)\n",
    "\n",
    "        return img, masked_img, aux\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a664b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize((128, 128), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset('ffhq',folders=folders, transforms_=transforms_),\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    ImageDataset('ffhq',folders=folders, transforms_=transforms_, mode=\"val\"),\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0219402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def downsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        def upsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.ReLU())\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *downsample(channels, 64, normalize=False),\n",
    "            *downsample(64, 64),\n",
    "            *downsample(64, 128),\n",
    "            *downsample(128, 256),\n",
    "            *downsample(256, 512),\n",
    "            nn.Conv2d(512, 4000, 1),\n",
    "            *upsample(4000, 512),\n",
    "            *upsample(512, 256),\n",
    "            *upsample(256, 128),\n",
    "            *upsample(128, 64),\n",
    "            nn.Conv2d(64, channels, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a89cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def save_sample(batches_done):\n",
    "    samples, masked_samples, i = next(iter(test_dataloader))\n",
    "    samples = Variable(samples.type(Tensor))\n",
    "    masked_samples = Variable(masked_samples.type(Tensor))\n",
    "    i = i[0].item()  # Upper-left coordinate of mask\n",
    "    # Generate inpainted image\n",
    "    gen_mask = generator(masked_samples)\n",
    "    filled_samples = masked_samples.clone()\n",
    "    filled_samples[:, :, i : i + 64, i : i + 64] = gen_mask\n",
    "    # Save sample\n",
    "    sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2)\n",
    "    save_image(sample, \"images/%d.png\" % batches_done, nrow=6, normalize=True)\n",
    "\n",
    "    \n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "pixelwise_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(channels=3)\n",
    "discriminator = Discriminator(channels=3)\n",
    "\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "adversarial_loss.cuda()\n",
    "pixelwise_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3a4d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_h, patch_w = int(64 / 2 ** 3), int(64 / 2 ** 3)\n",
    "patch = (1, patch_h, patch_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e7cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d660c4fe7984227aaf902f3a26ab872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a3e9630b854789b5d82b531c3af0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4649abfe1c9247488344ad1af914362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c96229c50b84570a84529624e507a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cff6bf4dd54be6beeb8e4db43d5873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bb63dda8be4b99bd290a29d786828d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550c14513e274ae192ea7e167c1b6a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07d9c08e0e44b57a9072735d60f13c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de8c8dd930f469db0550dce77b9ce05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fde384733349158e11710b929ca93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0b095fe963432baf6ac79b0729504b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104e0b9cdeb046d997c1bf0d26b69147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 11 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc38d06842d45db8a2d4e7f7dbbf6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 12 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f78201da1b40ba91124b17acfc8a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 13 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38939f0ef504acc91a8cdbd99b387a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 14 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f160383396d490a8ac6ef8de679d191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 15 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0290d697d40c45ae89a975732ecc4693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 16 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db530eb6533c4ff59c26f542cc290c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 17 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3082fa34164b4e768e5310500c310185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 18 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0540d731199b4dc281bdfcd03d51752a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 19 :   0%|          | 0/5500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_adv_losses, gen_pixel_losses, disc_losses, counter = [], [], [], []\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(50):\n",
    "    \n",
    "    ### Training ###\n",
    "    gen_adv_loss, gen_pixel_loss, disc_loss = 0, 0, 0\n",
    "    tqdm_bar = tqdm(dataloader, desc=f'Training Epoch {epoch} ', total=int(len(dataloader)))\n",
    "    for i, (imgs, masked_imgs, masked_parts) in enumerate(tqdm_bar):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], *patch).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], *patch).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        masked_imgs = Variable(masked_imgs.type(Tensor))\n",
    "        masked_parts = Variable(masked_parts.type(Tensor))\n",
    "\n",
    "        ## Train Generator ##\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_parts = generator(masked_imgs)\n",
    "\n",
    "        # Adversarial and pixelwise loss\n",
    "        g_adv = adversarial_loss(discriminator(gen_parts), valid)\n",
    "        g_pixel = pixelwise_loss(gen_parts, masked_parts)\n",
    "        # Total loss\n",
    "        g_loss = 0.001 * g_adv + 0.999 * g_pixel\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        ## Train Discriminator ##\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(masked_parts), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_parts.detach()), fake)\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        gen_adv_loss, gen_pixel_loss, disc_loss\n",
    "        gen_adv_losses, gen_pixel_losses, disc_losses, counter\n",
    "        \n",
    "        gen_adv_loss += g_adv.item()\n",
    "        gen_pixel_loss += g_pixel.item()\n",
    "        gen_adv_losses.append(g_adv.item())\n",
    "        gen_pixel_losses.append(g_pixel.item())\n",
    "        disc_loss += d_loss.item()\n",
    "        disc_losses.append(d_loss.item())\n",
    "        counter.append(i*12 + imgs.size(0) + epoch*len(dataloader.dataset))\n",
    "        tqdm_bar.set_postfix(gen_adv_loss=gen_adv_loss/(i+1), gen_pixel_loss=gen_pixel_loss/(i+1), disc_loss=disc_loss/(i+1))\n",
    "        \n",
    "        # Generate sample at sample interval\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % 1000 == 0:\n",
    "            save_sample(batches_done)\n",
    "    \n",
    "    if((0.001 * gen_adv_loss + 0.999 * gen_pixel_loss) < best_loss):\n",
    "        best_loss = 0.001 * gen_adv_loss + 0.999 * gen_pixel_loss\n",
    "        torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n",
    "        torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db480a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
